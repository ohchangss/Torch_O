{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac18125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8418198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, name, dataset):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        self.is_training = False\n",
    "        if not hasattr(self, 'rand_std'): self.rand_std = 0.030\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '{}/{}'.format(self.name, self.dataset)\n",
    "\n",
    "    def exec_all(self, epoch_count=10, batch_size=10, learning_rate=0.001,\n",
    "                 report=0, show_cnt=3, num_workers=0):\n",
    "        self.train(epoch_count, batch_size, learning_rate, report, num_workers)\n",
    "        self.test()\n",
    "        if show_cnt > 0: self.visualize(show_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f90bf686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp_Torch(Model):\n",
    "    def __init__(self, name, dataset, hconfigs):\n",
    "        self.layers=[]\n",
    "        super(Mlp_Torch, self).__init__(name, dataset)\n",
    "        self.make_layers(hconfigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6eee11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#우선 SLP 하나로 테스트 해보고 그 뒤 추후 늘린다.\n",
    "def mlp_make_layer_torch(self, hconfigs):\n",
    "    self.hconfig = hconfigs\n",
    "    \n",
    "    prev_shape = self.dataset.input_shape\n",
    "    \n",
    "    for hconfig in hconfigs:\n",
    "        #hconfig에는 output값이 들어있음.\n",
    "        _, prev_shape=self.alloc_make_layer(prev_shape, hconfig)\n",
    "        \n",
    "    output_cnt = int(np.prod(self.dataset.output_shape))\n",
    "    self.alloc_make_layer(prev_shape, output_cnt, atf=False)\n",
    "\n",
    "def mlp_alloc_make_layer_torch(self, input_shape, hconfig, atf=True):\n",
    "    \n",
    "    input_cnt = np.prod(input_shape)\n",
    "    output_cnt = np.prod(hconfig)\n",
    "    self.layers.append(nn.Linear(in_features=input_cnt,out_features=output_cnt))\n",
    "    if atf:self.layers.append(nn.ReLU())\n",
    "    if atf==False and self.dataset.mode =='binary':\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "    return input_cnt, output_cnt\n",
    "    \n",
    "    \n",
    "Mlp_Torch.make_layers = mlp_make_layer_torch\n",
    "Mlp_Torch.alloc_make_layer = mlp_alloc_make_layer_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9477b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc6b100e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Mlp_Torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14984/634667319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mMlp_Torch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_torch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Mlp_Torch' is not defined"
     ]
    }
   ],
   "source": [
    "def train_torch(self, epoch_count=10, batch_size=10, \\\n",
    "                    learning_rate=0.001, report=0, num_workers = 0):\n",
    "    \n",
    "    self.learning_rate = learning_rate\n",
    "    time1 = time2 = int(time.time())\n",
    "    \n",
    "    if report != 0:\n",
    "        print('Model {} train started'.format(self.name),'\\n'*2)\n",
    "    \n",
    "    \n",
    "    train_loader=self.dataset.dataloader(self.dataset.tr_xs, self.dataset.tr_ys, batch_size, num_workers)\n",
    "    self.model=Net(self.layers).to(DEVICE)\n",
    "#     self.model = model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(self.model.parameters(),lr=self.learning_rate)\n",
    "    if self.dataset.mode == 'regression':\n",
    "        loss_func = nn.MSELoss()\n",
    "        \n",
    "    elif self.dataset.mode == 'binary':\n",
    "        loss_func = nn.BCELoss()\n",
    "        \n",
    "    print('레이어는 아래와 같습니다.\\n\\n',self.model)\n",
    "    \n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        \n",
    "        for batch_idx, samples in enumerate(train_loader):\n",
    "\n",
    "            X_train, y_train = samples\n",
    "            \n",
    "            X_train=X_train.to(DEVICE)\n",
    "            y_train=y_train.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = self.model.forward(X_train)\n",
    "            loss = loss_func(output, y_train)\n",
    "            acc = self.eval_accuracy(X_train, y_train, output)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            costs.append(loss.cpu().detach().numpy())\n",
    "            accs.append(acc.cpu().detach().numpy())\n",
    "            \n",
    "            if report > 0 and (batch_idx+1) % report == 0:\n",
    "                va_X, va_Y = self.dataset.get_validate_date(100)\n",
    "                acc = self.eval_accuracy(va_X, va_Y)\n",
    "                time3 = int(time.time())\n",
    "                tm1, tm2 = time3 - time2, time3 - time1\n",
    "                self.dataset.train_prt_result(epoch+1, costs,accs,acc, tm1, tm2)\n",
    "                time2 = time3\n",
    "                \n",
    "    tm_total = int(time.time()) - time1\n",
    "    \n",
    "    print('Model {} train ended in {} secs:'.format(self.name, tm_total))\n",
    "\n",
    "            \n",
    "Mlp_Torch.train = train_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c12a4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_torch(self):\n",
    "    with torch.no_grad():\n",
    "        test_X,test_y=self.dataset.get_test_data()\n",
    "        time1 = int(time.time())        \n",
    "        acc = self.eval_accuracy(test_X, test_y)\n",
    "        time2 = int(time.time())\n",
    "        self.dataset.test_prt_result(self.name, acc, time2-time1)\n",
    "\n",
    "Mlp_Torch.test = test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efad862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_eval_accuracy_torch(self, x, y, output=None):\n",
    "    if output is None:\n",
    "        with torch.no_grad():\n",
    "            output = self.model.forward(x)\n",
    "    accuracy = self.dataset.eval_accuracy(x, y, output)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "Mlp_Torch.eval_accuracy = mlp_eval_accuracy_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06189be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.maximum(torch.tensor(0.5),torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_get_estimate_torch(self, x):\n",
    "    with torch.no_grad():\n",
    "        output = self.model.forward(x)\n",
    "    estimate = self.dataset.get_estimate(output)\n",
    "    return estimate\n",
    "\n",
    "Mlp_Torch.get_estimate = mlp_get_estimate_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_visualize_torch(self, num):\n",
    "    print('Model {} Visualization'.format(self.name))\n",
    "    deX, deY = self.dataset.get_visualize_data(num)\n",
    "    est = self.get_estimate(deX)\n",
    "    self.dataset.visualize(deX, est, deY)\n",
    "\n",
    "Mlp_Torch.visualize = mlp_model_visualize_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4fc3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
