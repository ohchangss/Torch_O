{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1e8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../'*3\n",
    "RUN_DIR = BASE_DIR + 'code/CNN/cnn_regmodel/cnn_reg_model.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c84b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run {RUN_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnExtModel(CnnRegModel):\n",
    "    macros ={}\n",
    "    \n",
    "    def __init__(self, name, dataset, hconfigs, show_maps=False,\\\n",
    "                l2_decay = 0, l1_decay = 0, dump_structure = False):\n",
    "        self.dump_structure = dump_structure\n",
    "        self.layer_index = 0\n",
    "        self.layer_depth=0\n",
    "        self.param_count = 0\n",
    "        #\n",
    "#         self.modules = nn.Sequential()\n",
    "#         self.modules = nn.Sequential()\n",
    "        self.modules = {}\n",
    "        self.net = []\n",
    "        self.name = name\n",
    "        self.macro =[]\n",
    "        self.idx = 1\n",
    "        #\n",
    "        super(CnnExtModel, self).__init__(name, dataset, hconfigs, show_maps,l2_decay, l1_decay)\n",
    "        if self.dump_structure :\n",
    "            print('Total parameter count : {}'.format(self.param_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d737d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef72089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_layer_param(self, input_shape, hconfig):\n",
    "    layer_type = get_layer_type(hconfig)\n",
    "    if layer_type in ['serial', 'parallel', 'loop','add','custom']:\n",
    "        if self.dump_structure:\n",
    "            dump_str = layer_type\n",
    "            \n",
    "            if layer_type =='custom':\n",
    "                name = get_conf_param(hconfig, 'name')\n",
    "                dump_str += ' '+name\n",
    "            print('{:>{width}}{}'.format('', dump_str, width = self.layer_depth*2))\n",
    "        self.layer_depth += 1;\n",
    "    pm, output_shape = super(CnnExtModel, self).alloc_layer_param(input_shape, hconfig)\n",
    "    \n",
    "    if layer_type in ['serial', 'parallel', 'loop', 'add', 'custom']:\n",
    "        self.layer_depth -= 1;\n",
    "        \n",
    "    elif self.dump_structure:\n",
    "        self.layer_index +=1\n",
    "        pm_str ='';\n",
    "        if layer_type == 'full':\n",
    "#             print(x)\n",
    "            ph, pw = pm[0].weight.shape\n",
    "            pm_count = torch.prod(torch.tensor(pm[0].weight.shape)) + torch.tensor(pm[0].bias.shape[0])\n",
    "            self.param_count += pm_count\n",
    "            pm_str = 'pm :{}x{}+{}={}'.format(ph,pw,torch.tensor(pm[0].bias.shape[0]),pm_count)\n",
    "\n",
    "        elif layer_type == 'conv':\n",
    "                    \n",
    "            ychn,xchn,kh,kw = pm[0].weight.shape\n",
    "            pm_count= torch.prod(torch.tensor(pm[0].weight.shape))+torch.tensor(pm[0].bias.shape[0])\n",
    "            self.param_count += pm_count\n",
    "            \n",
    "            \n",
    "            pm_str = 'pm : {}x{}x{}x{}+{} = {}'.format(ychn, xchn, kh, kw, torch.tensor(pm[0].bias.shape[0]), pm_count)\n",
    "        print('{:>{width}}{}: {}, {}=>{}{}'.format('', self.layer_index, layer_type, input_shape, output_shape, pm_str,width=self.layer_depth*2))\n",
    "    \n",
    "    return pm, output_shape\n",
    "\n",
    "CnnExtModel.alloc_layer_param = cnn_ext_alloc_layer_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_parallel_layer(self, input_shape, hconfig):\n",
    "#     pm_hiddens_parallel = nn.Sequential()\n",
    "    pm_hiddens_parallel = []\n",
    "\n",
    "    num_parallel=0\n",
    "    output_shape =None\n",
    "    \n",
    "    if not isinstance(hconfig[1], dict): hconfig.insert(1,{})\n",
    "            \n",
    "        \n",
    "    for bconfig in hconfig[2:]:\n",
    "\n",
    "        bpm, bshape = self.alloc_layer_param(input_shape, bconfig)\n",
    "        if bpm!=None:\n",
    "            pm_hiddens_parallel.append(bpm[0])\n",
    "\n",
    "#             pm_hiddens_parallel.add_module('parallel_{0}'.format(self.layer_index), bpm[0])\n",
    "            num_parallel += 1\n",
    "        if output_shape:\n",
    "            assert output_shape[1:] == bshape[1:]\n",
    "            output_shape[0] += bshape[0]\n",
    "            \n",
    "        else:\n",
    "            output_shape = bshape\n",
    "            \n",
    "        \n",
    "    return [pm_hiddens_parallel], output_shape\n",
    "\n",
    "\n",
    "CnnExtModel.alloc_parallel_layer = cnn_ext_alloc_parallel_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_serial_layer(self, input_shape, hconfig):\n",
    "#     pm_hiddens_serial =nn.Sequential()\n",
    "    pm_hiddens_serial =[]\n",
    "    prev_shape = input_shape\n",
    "    if not isinstance(hconfig[1], dict) : hconfig.insert(1,{})\n",
    "    num_serial=0\n",
    "    for sconfig in hconfig[2:]:\n",
    "        \n",
    "        pm_hidden, prev_shape = self.alloc_layer_param(prev_shape,sconfig)\n",
    "#         print(pm_hidden)\n",
    "#         exit()\n",
    "        if pm_hidden != None:\n",
    "            pm_hiddens_serial.append(pm_hidden[0])\n",
    "#             pm_hiddens_serial.add_module('serial_{0}'.format(self.layer_index), pm_hidden[0])\n",
    "#             self.serial.add_module('{0}'.format(self.num), pm_hidden[0])\n",
    "            num_serial += 1\n",
    "    return [pm_hiddens_serial], prev_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CnnExtModel.alloc_serial_layer = cnn_ext_alloc_serial_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_add_layer(self, input_shape, hconfig):\n",
    "    if not isinstance(hconfig[1], dict): hconfig.insert(1, {})\n",
    "    \n",
    "    bpm, output_shape = self.alloc_layer_param(input_shape, hconfig[2])\n",
    "    pm_hiddens = [bpm] \n",
    "    \n",
    "    for bconfig in hconfig[3:]:\n",
    "        bpm, bshape =self.alloc_layer_param(input_shape, bconfig)\n",
    "        pm_hiddens.append(bpm)\n",
    "        check_add_shapes(output_shape, bshape)\n",
    "        \n",
    "    if get_conf_param(hconfig, 'x', True):\n",
    "        check_add_shape(output_shape, input_shape)\n",
    "        \n",
    "    pm = {'pms':pm_hiddens}\n",
    "    \n",
    "    for act in get_conf_param(hconfig, 'actions', ''):\n",
    "        if act == 'B':\n",
    "            bn_config = ['batch_normal', {'rescale':True}]\n",
    "            pm['bn'], _ =self.alloc_batch_normal_param(output_shape, bn_config)\n",
    "            \n",
    "    return pm, output_shape\n",
    "\n",
    "CnnExtModel.alloc_add_layer = cnn_ext_alloc_add_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62969f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cnn_ext_forward_add_layer(self, x, hconfig, pm):\n",
    "#     y, baux = self.forward_layer(x, hconfig[2], pm['pms'][0])\n",
    "#     bauxes, bchns, aux_bn = [baux], [y.shape[-1]], [] \n",
    "    \n",
    "#     for n, bconfig in enumerate(hconfig[3:]):\n",
    "#         by, baux = self.forward_layer(x, bconfig, pm['pms'][n+1])\n",
    "#         y += tile_add_result(by, y.shape[-1], by.shape[-1])\n",
    "        \n",
    "#         bauxes.append(baux)\n",
    "#         bchns.append(by.shape[-1])\n",
    "        \n",
    "        \n",
    "#     if get_conf_param(hconfig, 'x', True):\n",
    "#         y += tile_add_result(x, y.shape[-1], x.shape[-1])\n",
    "        \n",
    "#     for act in get_conf_param(hconfig, 'actions',''):\n",
    "#         if act == 'A' : y = self.activate(y, hconfig)\n",
    "#         if act == 'B':\n",
    "#             y, aux_bn = self.forward_batch_normal_layer(y, None, pm['bn'])\n",
    "            \n",
    "            \n",
    "#     return y, [y, bauxes, bchns, aux_bn, x.shape]\n",
    "\n",
    "\n",
    "# CnnExtModel.forward_add_layer = cnn_ext_forward_add_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cnn_ext_backprop_add_layer(self, G_y, hconfig, pm, aux):\n",
    "#     y, bauxes, bchns, aux_bn, x_shape = aux\n",
    "    \n",
    "#     for act in reversed(get_conf_param(hconfig, 'actions','')):\n",
    "#         if act == 'A' : G_y = self.activate_derv(G_y, y, hconfig)\n",
    "#         if act == 'B':\n",
    "#             G_y = self.backprop_batch_normal_layer(G_y, None, pm['bn'], aux_bn)\n",
    "            \n",
    "#     G_x = np.zeros(x_shape)\n",
    "    \n",
    "#     for n, bconfig in enumerate(hconfig[2:]):\n",
    "#         G_by = merge_add_grad(G_y, G_y.shape[-1], bchns[n])\n",
    "#         G_x += self.backprop_layer(G_by, bconfig, pm['pms'][n], bauxes[n])\n",
    "        \n",
    "#     if get_conf_param(hconfig, 'x', True):\n",
    "#         G_x += merge_add_grad(G_y, G_y.shape[-1], x_shape[-1])\n",
    "        \n",
    "#     return G_x\n",
    "\n",
    "\n",
    "# CnnExtModel.backprop_add_layer = cnn_ext_backprop_add_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d409d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_add_shapes(yshape, bashpe):\n",
    "    assert yshape[:-1] == bshape[:-1]\n",
    "    assert yshape[-1] % bshape[-1] == 0\n",
    "    \n",
    "def tile_add_result(by, ychn, bchn):\n",
    "    if ychn == bchn : return by\n",
    "    times = ychn // bchn\n",
    "    return np.tile(by, times)\n",
    "\n",
    "def merge_add_grad(G_y, ychn, bchn):\n",
    "    if ychn == bchn: return G_y\n",
    "    \n",
    "    times = ychn // bchn\n",
    "    split_shape = G_y.shape[:-1] + tuple([times, bchn])\n",
    "    return np.sum(G_y.reshape(split_shape), axis = -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_loop_layer(sefl, input_shape, hconfig):\n",
    "    pm_hiddens = []\n",
    "    prev_shape = input_shape\n",
    "    \n",
    "    if not isinstance(hconfig[1], dict): hconfig.insert(1,{})\n",
    "        \n",
    "    for n in range(get_conf_param(hconfig, 'repeat',1)):\n",
    "        pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig[2])\n",
    "        pm_hiddens.append(pm_hidden)\n",
    "        \n",
    "    return {'pms':pm_hiddens},prev_shape\n",
    "\n",
    "\n",
    "def cnn_ext_forward_loop_layer(self, x, hconfig, pm):\n",
    "    hidden = x\n",
    "    aux_layers = []\n",
    "    \n",
    "    for n in range(get_conf_param(hconfig, 'repeat', 1)):\n",
    "        hidden, aux = self.forward_layer(hidden, hconfig[2], pm['pms'][n])\n",
    "        aux_layers.append(aux)\n",
    "        \n",
    "    return hidden, aux_layers\n",
    "\n",
    "\n",
    "CnnExtModel.alloc_loop_layer = cnn_ext_alloc_loop_layer\n",
    "CnnExtModel.alloc_forward_layer = cnn_ext_forward_loop_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc2c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223814e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_custom_layer(self, input_shape, hconfig):\n",
    "    \n",
    "    name = get_conf_param(hconfig, 'name')\n",
    "    args = get_conf_param(hconfig, 'args',{})\n",
    "    macro = CnnExtModel.get_macro(name, args)\n",
    "    print(len(macro))\n",
    "    pm_hidden, output_shape = self.alloc_layer_param(input_shape, macro)\n",
    "    assert isinstance(macro[1], dict)\n",
    "    \n",
    "#     module=self.get_module(name, pm_hidden, macro):\n",
    "#     print()    \n",
    "#     self.modules.add_module(name+'_{0}'.format(self.idx), pm_hidden[0])\n",
    "    col = name+'_{0}'.format(self.idx)\n",
    "    self.modules[col]= pm_hidden[0]\n",
    "    \n",
    "    if len(macro)==self.idx+1:\n",
    "        pass\n",
    "    elif name.split('_')[-1] == 'preproc' and 'postproc':\n",
    "#         print(type(name),name,'namenamenamenamenamenamenamenamenamenamenamename')\n",
    "#         print(x)\n",
    "\n",
    "        self.net.append(nn.Sequential(*pm_hidden[0]))\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        mo_dict=self.get_module_dict(pm_hidden[0], col)\n",
    "        B=Block(pm_hidden[0], col, mo_dict)\n",
    "        self.net.append(B)\n",
    "    self.macro.append(macro)\n",
    "    \n",
    "    self.idx += 1\n",
    "    \n",
    "    return self.net, output_shape\n",
    "\n",
    "def cnn_ext_forward_custom_layer(self, x, hconfig, pm):\n",
    "    return self.forward_layer(x, pm['macro'], pm['pm'])\n",
    "\n",
    "\n",
    "\n",
    "CnnExtModel.alloc_custom_layer = cnn_ext_alloc_custom_layer\n",
    "CnnExtModel.forward_custom_layer = cnn_ext_forward_custom_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_get_module_dict(self,pm, col):\n",
    "    dic={}\n",
    "    idx = 1\n",
    "    for i in pm:\n",
    "        if not isinstance(i,list):\n",
    "            print(type(col),col)\n",
    "            print(idx)\n",
    "            print(i)\n",
    "            dic[col+'_'+str(idx)]=i\n",
    "            idx += 1\n",
    "        else:\n",
    "            for j in i:\n",
    "                if not isinstance(j, list):\n",
    "                    dic[col+'_'+str(idx)]=j\n",
    "                    idx +=1\n",
    "                else:\n",
    "                    k,m = j\n",
    "                    dic[col+'_'+str(idx)]=k\n",
    "                    idx +=1\n",
    "                    dic[col+'_'+str(idx)]=m\n",
    "                    idx +=1\n",
    "    return dic\n",
    "    \n",
    "CnnExtModel.get_module_dict = cnn_ext_get_module_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d45b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, modules, col, module_dict):\n",
    "        \n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        self.col = col\n",
    "        self.modules = modules\n",
    "        \n",
    "        setattr(self, self.col, nn.Sequential())\n",
    "        method = getattr(self, self.col)\n",
    "        method.add_module('dict', nn.ModuleDict(module_dict))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        cat_modules=[]\n",
    "        idx = 1\n",
    "        print(self.modules)\n",
    "        for i in self.modules:\n",
    "            if not isinstance(i,list):\n",
    "                print(i)\n",
    "                print(idx)\n",
    "                print(self.col)\n",
    "                out,idx = self.get_output(idx,x)\n",
    "                cat_modules.append(out)\n",
    "            else:\n",
    "                out_val = None\n",
    "                for j in i:\n",
    "                    if not isinstance(j, list):\n",
    "                        out,idx = self.get_output(idx, x)\n",
    "                        out_val=out\n",
    "                    else:\n",
    "                        out,idx = self.get_output(idx,out_val)\n",
    "                        cat_modules.append(out)\n",
    "        print(torch.cat(cat_modules,dim=1).shape,'222222222222222222')\n",
    "        return torch.cat(cat_modules,dim=1)                \n",
    "            \n",
    "    def get_output(self,idx,x):\n",
    "        out_method = getattr(self, self.col)\n",
    "        if isinstance(x, list):\n",
    "            val1, val2=x\n",
    "            val1 = out_method.dict[self.col+'_'+ str(idx)](x)\n",
    "            idx += 1\n",
    "            val2 = out_method.dict[self.col+'_' + str(idx)](x)\n",
    "            idx += 1\n",
    "            print(val1.shape,val2.shape,'1111111111111111111111111111111')\n",
    "            return torch.cat([val1,val2],dim=1),idx\n",
    "        \n",
    "        else:\n",
    "            out = out_method.dict[self.col+'_' + str(idx)](x)\n",
    "            print(out.shape)\n",
    "            idx += 1\n",
    "\n",
    "            return out,idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_set_macro(name, config):\n",
    "    CnnExtModel.macros[name] = config\n",
    "    \n",
    "def cnn_ext_get_macro(name, args):\n",
    "    \n",
    "    restored = copy.deepcopy(CnnExtModel.macros[name])\n",
    "    replace_arg(restored, args)\n",
    "    \n",
    "    return restored\n",
    "\n",
    "def replace_arg(exp, args):\n",
    "    if isinstance(exp, (list, tuple)):\n",
    "        for n, term in enumerate(exp):\n",
    "            if isinstance(term, str) and term[0] == '#':\n",
    "                if term[1] == '#' :exp[n]= term[1:]\n",
    "                elif term in args: exp[n]= args[term]\n",
    "                    \n",
    "            else:\n",
    "                replace_arg(term, args)\n",
    "                \n",
    "    elif isinstance(exp, dict):\n",
    "        for key in exp:\n",
    "            if isinstance(exp[key], str) and exp[key][0] == '#':\n",
    "                if exp[key][1]=='#' :exp[key] = exp[key][1:]\n",
    "                elif exp[key] in args : exp[key] = args[exp[key]]\n",
    "                    \n",
    "            else:\n",
    "                replace_arg(exp[key], args)\n",
    "                \n",
    "CnnExtModel.set_macro = cnn_ext_set_macro\n",
    "CnnExtModel.get_macro = cnn_ext_get_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_conv_layer(self, input_shape, hconfig):\n",
    "    pm, output_shape = super(CnnExtModel, self).alloc_conv_layer(input_shape, hconfig)\n",
    "    \n",
    "    # 상속을 통하여 기존의 것들을 사용하고 추가되는 부분을 추가\n",
    "    act_func = get_conf_param(hconfig, 'actions','LA')\n",
    "    \n",
    "    for act in act_func:\n",
    "        \n",
    "        if act == 'L':\n",
    "            input_shape = output_shape\n",
    "        elif act == 'B':\n",
    "            bn_config=['batch_noram', {'rescale':False}]\n",
    "            pm['bn'], _ = self.alloc_batch_normal_layer(input_shape, bn_config)\n",
    "            \n",
    "            \n",
    "    xchn, xh, xw = input_shape\n",
    "    \n",
    "    ychn= get_conf_param(hconfig, 'chn')\n",
    "    \n",
    "    output_shape = eval_stride_shape(hconfig, True, ychn, xh, xw)\n",
    "#     self.serial.add_module('{0}'.format(self.num),pm[0])\n",
    "    return pm, output_shape\n",
    "\n",
    "\n",
    "CnnExtModel.alloc_conv_layer = cnn_ext_alloc_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b90742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_forward_conv_layer(self, x, hconfig, pm):\n",
    "    y = x\n",
    "    x_flat, k_flat, relu_y, aux_bn = None, None, None, None\n",
    "    \n",
    "    for act in pm['actions']:\n",
    "        if act == 'L':\n",
    "            mb_size, xh, xw, xchn = y.shape\n",
    "            kh, kw, _, ychn = pm['k'].shape\n",
    "            x_flat = get_ext_regions_for_conv(y, kh, kw)\n",
    "            k_flat = pm['k'].reshape([kh*kw*xchn, ychn])\n",
    "            conv_flat = np.matmul(x_flat, k_flat)\n",
    "            y=conv_flat.reshape([mb_size, xh, xw, ychn])+pm['b']\n",
    "            \n",
    "        elif act == 'A':\n",
    "            y= self.activate(y, hconfig)\n",
    "            relu_y = y\n",
    "            \n",
    "        elif act =='B':\n",
    "            y, aux_bn = self.forward_batch_normal_layer(y, None, pm['bn'])\n",
    "            \n",
    "    y, aux_stride = stride_filter(hconfig, True, y)\n",
    "    \n",
    "    if self.need_maps: self. maps.append(y)\n",
    "        \n",
    "    return y, [x_flat, k_flat, x, relu_y, aux_bn, aux_stride]\n",
    "\n",
    "\n",
    "CnnExtModel.forward_conv_layer = cnn_ext_forward_conv_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a29bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_max_layer(self, input_shape, hconfig):\n",
    "    ychn, xh, xw = input_shape \n",
    "    output_shape = eval_stride_shape(hconfig, False, ychn, xh, xw )\n",
    "    kh, kw, sh, sw, padding = get_shape_params(hconfig, False)\n",
    "    \n",
    "    if padding =='VALID':padding='valid'\n",
    "    if padding =='SAME':padding='same'\n",
    "        \n",
    "    pm=nn.MaxPool2d((kh,kw),stride=(sh,sw),padding=padding)\n",
    "    \n",
    "    return [pm], output_shape\n",
    "\n",
    "\n",
    "CnnExtModel.alloc_max_layer = cnn_ext_alloc_max_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_forward_max_layer(self, x, hconfig, pm):\n",
    "    \n",
    "    mb_size, xh, xw, chn = x.shape\n",
    "    \n",
    "    sh, sw = get_conf_param_2d(hconfig, 'strid', [1,1])\n",
    "    kh, kw = get_conf_param_2d(hconfig, 'ksize', [sh, sw])\n",
    "    padding = get_conf_param(hconfig, 'padding', 'SAME')\n",
    "    \n",
    "    if [sh, sw] == [kh, kw] and xh % sh == 0 and xw % sw == 0 and \\\n",
    "    padding == 'SAME':\n",
    "        return super(CnnExtModel, self).forward_max_layer(x, hconfig, pm)\n",
    "    \n",
    "    x_flat = get_ext_regions(x, kh, kw, -np.inf)\n",
    "    x_flat = x_flat.transpose([2, 5, 0, 1, 3, 4])\n",
    "    x_flat = x_flat.reshape(mb_size * chn * xh * xw, kh*kw)\n",
    "    \n",
    "    max_idx = np.argmax(x_flat, axis = 1)\n",
    "    y = x_flat[np.arange(x_flat.shape[0]), max_idx]\n",
    "    y = y.reshape([mb_size, chn, xh, xw])\n",
    "    y = y.transpose([0,2,3,1])\n",
    "    \n",
    "    \n",
    "    y, aux_stride = stride_filter(hconfig, False, y)\n",
    "    \n",
    "    if self.need_maps : self.maps.append(y)\n",
    "    \n",
    "    return y, [x.shape, kh, kw, sh, sw, padding, max_idx, aux_stride]\n",
    "\n",
    "\n",
    "\n",
    "CnnExtModel.forward_max_layer = cnn_ext_forward_max_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ee346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_stride_shape(hconfig, conv_type, ychn, xh, xw):\n",
    "    kh, kw, sh, sw, padding = get_shape_params(hconfig, conv_type)\n",
    "    \n",
    "    if padding == 'VALID':\n",
    "        xh = xh - kh + 1\n",
    "        xw = xw - kw + 1\n",
    "        \n",
    "    yh = xh // sh\n",
    "    yw = xw // sw\n",
    "    \n",
    "    return [ychn, yh, yw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75dc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_filter(hconfig, conv_type, y):\n",
    "    \n",
    "    _, xh, xw, _ = x_shape=y.shape\n",
    "    nh, nw = xh, xw\n",
    "    kh, kw, sh, sw, padding = get_shape_params(hconfig, conv_type)\n",
    "    \n",
    "    \n",
    "    if padding == 'VALID':\n",
    "        bh, bw = (kh -1)//2, (kw-1)//2\n",
    "        nh, nw = xh - kh +1, xw - kw +1\n",
    "        y=y[:,bh:bh+nh, bw:bw+nw:, :]\n",
    "        \n",
    "        \n",
    "    if sh != 1 or sw != 1:\n",
    "        bh, bw = (sh-1)//2, (sw -1)//2\n",
    "        mh,mw = nh // sh, nw//sw\n",
    "        y = y[:,bh:bh+mh*sh:sh, bw:bw+mw*sw:sw, :]\n",
    "        \n",
    "        \n",
    "    return y, [x_shape, nh, nw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_params(hconfig, conv_type):\n",
    "    if conv_type:\n",
    "        kh, kw = get_conf_param_2d(hconfig, 'ksize')\n",
    "        sh, sw = get_conf_param_2d(hconfig, 'stride',[1,1])\n",
    "        \n",
    "    else:\n",
    "        sh,sw = get_conf_param_2d(hconfig, 'stride', [1,1])\n",
    "        kh,kw = get_conf_param_2d(hconfig, 'ksize',[sh, sw])\n",
    "        \n",
    "    padding=get_conf_param(hconfig, 'padding', 'SAME')\n",
    "#     print(padding)\n",
    "    return kh, kw, sh, sw, padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b61f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_module(self,pm,name):\n",
    "    \n",
    "#     layer_type = get_layer_type(hconfig)\n",
    "    \n",
    "#     if name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e030f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c2c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bdf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
