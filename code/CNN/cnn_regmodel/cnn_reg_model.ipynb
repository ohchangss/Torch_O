{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c1b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../'*3\n",
    "RUN_DIR = BASE_DIR+\"code/CNN/cnn_basic/cnn_basic_model.ipynb\"\n",
    "%run {RUN_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a2d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnRegModel(CnnBasicModel):\n",
    "    def __init__(self, name, dataset, hconfigs, show_maps=False,\n",
    "                l2_decay=0, l1_decay=0):\n",
    "        self.l2_decay = l2_decay\n",
    "        self.l1_decay = l1_decay\n",
    "        super(CnnBasicModel, self).__init__(name, datset, hconfigs, show_maps)\n",
    "        \n",
    "    def exec_all(self, epoch_count=10, batch_size = 10, learning_rate = 0.001,\n",
    "                 report=0, show_cnt = 3, show_params=False):\n",
    "        super(CnnRegModel, self).exec_all(epoch_count, batch_size, \n",
    "                                         learning_rate, report, show_cnt)\n",
    "        \n",
    "        if show_params : self.show_param_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c973a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reg_show_param_dist(self):\n",
    "    params = self.collect_params()\n",
    "    mu = np.mean(params)\n",
    "    sigma = np.sqrt(np.var(params))\n",
    "    plt.hist(params, 100, density=True, facecolor='g', alpha=0.75)\n",
    "    plt.axis([-0.2, 0.2, 0, 20.0])\n",
    "    plt.text(0.08, 15.0, 'mu={:5.3f}'.format(mu))\n",
    "    plt.text(0.08, 13.0, 'sigma={:5.3f}'.format(sigma))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    total_count = len(params)\n",
    "    near_zero_count = len(list(x for x in params if -1e-5 <= x <= 1e-5))\n",
    "    print('Near 0 parameters = {:4.1f}%({}/{})'.\n",
    "        format(near_zero_count/total_count*100, near_zero_count, total_count))\n",
    "\n",
    "def cnn_reg_collect_params(self):\n",
    "    params = list(self.pm_output['w'].flatten())\n",
    "    for pm in self.pm_hiddens:\n",
    "        if 'w' in pm: params += list(pm['w'].flatten())\n",
    "        if 'k' in pm: params += list(pm['k'].flatten())\n",
    "    return params\n",
    "\n",
    "CnnRegModel.show_param_dist = cnn_reg_show_param_dist\n",
    "CnnRegModel.collect_params = cnn_reg_collect_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d89b0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class NN_Network(nn.Module):\n",
    "    def __init__(self,in_dim,hid,out_dim):\n",
    "        super(NN_Network, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_dim,hid)\n",
    "        self.linear2 = nn.Linear(hid,out_dim)\n",
    "#         self.linear1.weight = torch.nn.Parameter(torch.zeros(in_dim,hid))\n",
    "#         self.linear1.bias = torch.nn.Parameter(torch.ones(hid))\n",
    "#         self.linear2.weight = torch.nn.Parameter(torch.zeros(in_dim,hid))\n",
    "#         self.linear2.bias = torch.nn.Parameter(torch.ones(hid))\n",
    "\n",
    "    def forward(self, input_array):\n",
    "        h = self.linear1(input_array)\n",
    "        y_pred = self.linear2(h)\n",
    "        return y_pred\n",
    "\n",
    "in_d = 5\n",
    "hidn = 2\n",
    "out_d = 3\n",
    "net = NN_Network(in_d, hidn, out_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521dd521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3439, -0.0863, -0.1446, -0.1207, -0.1956],\n",
       "         [ 0.2121, -0.0736,  0.2328,  0.2749,  0.4164]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.3716, -0.0986], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.3138, -0.2537],\n",
       "         [-0.5405,  0.1145],\n",
       "         [-0.4519,  0.1234]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1042, -0.0980, -0.3883], requires_grad=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05c7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f48ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optim(self, use_adam):\n",
    "    \n",
    "    if use_adam:\n",
    "        optim=torch.optim.Adam(self.model.parameters(),lr=self.learning_rate,\\\n",
    "                         betas=(0.9,0.999),eps=1e-08, weight_decay=self.l2_decay)\n",
    "    \n",
    "    else :\n",
    "        optim=torch.optim.SGD(self.model.parameters(),lr = self.learning_rate,weight_decay=self.l2_decay)\n",
    "        \n",
    "    \n",
    "    return optim\n",
    "\n",
    "CnnRegModel.get_optim = get_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe80420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGD in module torch.optim.sgd:\n",
      "\n",
      "class SGD(torch.optim.optimizer.Optimizer)\n",
      " |  SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
      " |  \n",
      " |  Implements stochastic gradient descent (optionally with momentum).\n",
      " |  \n",
      " |  .. math::\n",
      " |     \\begin{aligned}\n",
      " |          &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      " |          &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
      " |              \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
      " |          &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\\:nesterov\\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      " |          &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
      " |          &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
      " |          &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n",
      " |          &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
      " |          &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n",
      " |          &\\hspace{10mm}\\textbf{else}                                                          \\\\\n",
      " |          &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n",
      " |          &\\hspace{10mm}\\textbf{if} \\: nesterov                                                \\\\\n",
      " |          &\\hspace{15mm} g_t \\leftarrow g_{t-1} + \\mu \\textbf{b}_t                             \\\\\n",
      " |          &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n",
      " |          &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n",
      " |          &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                    \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      " |          &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      " |     \\end{aligned}\n",
      " |  \n",
      " |  Nesterov momentum is based on the formula from\n",
      " |  `On the importance of initialization and momentum in deep learning`__.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
      " |          parameter groups\n",
      " |      lr (float): learning rate\n",
      " |      momentum (float, optional): momentum factor (default: 0)\n",
      " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      " |      dampening (float, optional): dampening for momentum (default: 0)\n",
      " |      nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
      " |  \n",
      " |  Example:\n",
      " |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      " |      >>> optimizer.zero_grad()\n",
      " |      >>> loss_fn(model(input), target).backward()\n",
      " |      >>> optimizer.step()\n",
      " |  \n",
      " |  __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      " |  \n",
      " |  .. note::\n",
      " |      The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      " |      Sutskever et. al. and implementations in some other frameworks.\n",
      " |  \n",
      " |      Considering the specific case of Momentum, the update can be written as\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n",
      " |      parameters, gradient, velocity, and momentum respectively.\n",
      " |  \n",
      " |      This is in contrast to Sutskever et. al. and\n",
      " |      other frameworks which employ an update of the form\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - v_{t+1}.\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      The Nesterov version is analogously modified.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Args:\n",
      " |          closure (callable, optional): A closure that reevaluates the model\n",
      " |              and returns the loss.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_param_group(self, param_group)\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      " |      trainable and added to the :class:`Optimizer` as training progresses.\n",
      " |      \n",
      " |      Args:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along with group\n",
      " |          specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): optimizer state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * state - a dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes.\n",
      " |      * param_groups - a list containing all parameter groups where each\n",
      " |          parameter group is a dict\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False)\n",
      " |      Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              This will in general have lower memory footprint, and can modestly improve performance.\n",
      " |              However, it changes certain behaviors. For example:\n",
      " |              1. When the user tries to access a gradient and perform manual ops on it,\n",
      " |              a None attribute or a Tensor full of 0s will behave differently.\n",
      " |              2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
      " |              are guaranteed to be None for params that did not receive a gradient.\n",
      " |              3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
      " |              (in one case it does the step with a gradient of 0 and in the other it skips\n",
      " |              the step altogether).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb00a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
